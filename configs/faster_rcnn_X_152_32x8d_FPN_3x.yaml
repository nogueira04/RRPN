_BASE_: "Base-RCNN-FPN.yaml"
MODEL:
  MASK_ON: False
  WEIGHTS: "X-152-32x8d-IN5k.pkl"
  PIXEL_STD: [57.375, 57.120, 58.395]
  RESNETS:
    STRIDE_IN_1X1: False  # this is a C2 model
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
    DEPTH: 101
    

SOLVER:
  STEPS: (10000, 35000, 75000, 100000, 135000)      # Adjusted learning rate decay steps to smooth transitions
  MAX_ITER: 150000           # Slightly increased maximum iterations for more gradual training
  BASE_LR: 0.0008825170673330199           # Lowered base learning rate for more stable gradient updates
  WEIGHT_DECAY: 7.47537612951795e-05       # Increased weight decay to prevent overfitting and reduce variability
  LR_SCHEDULER_NAME: "WarmupCosineLR"  # Smoother learning rate schedule with cosine annealing
  WARMUP_FACTOR: 0.001       # Factor for gradual warmup
  WARMUP_ITERS: 1000         # Number of warmup iterations
  WARMUP_METHOD: "linear"    # Linear warmup for smoother transition to base learning rate
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_VALUE: 1.0          # Gradient clipping to stabilize gradient updates
  IMS_PER_BATCH: 4          # Adjust batch size if needed (depends on your GPU memory)
  OPTIMIZER: "AdamW"         # AdamW optimizer for more stable learning
  MOMENTUM: 0.9882741500174196              # Only used if switching back to SGD
  WEIGHT_DECAY_NORM: 0.0001  # Optional additional regularization for norm layers
  GAMMA: 0.1                 # Factor for reducing the learning rate at STEPS
  SCHEDULER_STEPS: (10000, 35000, 75000, 100000, 135000)  # Ensure scheduler aligns with learning rate steps